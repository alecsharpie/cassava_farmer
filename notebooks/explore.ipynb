{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73a45746",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (2.7.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (12.0.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (1.34.1)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (1.19.2)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (0.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (0.22.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (49.2.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd67f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285b5745",
   "metadata": {},
   "source": [
    "# organise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7274f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  1000015157.jpg      0\n",
       "1  1000201771.jpg      3\n",
       "2   100042118.jpg      1\n",
       "3  1000723321.jpg      1\n",
       "4  1000812911.jpg      3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../raw_data/cassava-leaf-disease-classification/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3bfd152",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(df.label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6108f0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'Cassava Bacterial Blight (CBB)',\n",
       " '1': 'Cassava Brown Streak Disease (CBSD)',\n",
       " '2': 'Cassava Green Mottle (CGM)',\n",
       " '3': 'Cassava Mosaic Disease (CMD)',\n",
       " '4': 'Healthy'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "label_map = json.load(open('../raw_data/cassava-leaf-disease-classification/label_num_to_disease_map.json'))\n",
    "label_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e49cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'0': 'cassava_bacterial_blight',\n",
    " '1': 'cassava_brown_streak_disease',\n",
    " '2': 'cassava_green_mottle',\n",
    " '3': 'cassava_mosaic_disease',\n",
    " '4': 'healthy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be8a25a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label_text'] = df.label.astype(str).map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dd803b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once to reorganise images into folders of their class\n",
    "# create new folders\n",
    "#for label in label_map.values():\n",
    "#    os.mkdir(f'../raw_data/cassava-leaf-disease-classification/train_images/{label}')\n",
    "\n",
    "# for idx, row in df.iterrows():\n",
    "\n",
    "#     old_path = f\"../raw_data/cassava-leaf-disease-classification/train_images/{row['image_id']}\"\n",
    "#     new_path = f\"../raw_data/cassava-leaf-disease-classification/train_images/{row['label_text']}/{row['image_id']}\"\n",
    "    \n",
    "#     os.rename(old_path, new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f422353e",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d1c59ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list = df.sort_values('image_id').label.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8533cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21397 files belonging to 5 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 00:59:12.958431: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#'/Users/alecsharp/me/cassava_farmer/raw_data/cassava-leaf-disease-classification/train_images'\n",
    "\n",
    "train_path = '../raw_data/cassava-leaf-disease-classification/train_images'\n",
    "\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "ds = image_dataset_from_directory(\n",
    "    train_path, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdc794a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21397 files belonging to 5 classes.\n",
      "Using 17118 files for training.\n",
      "Found 21397 files belonging to 5 classes.\n",
      "Using 4279 files for validation.\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(path, subset):\n",
    "    return tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        path,\n",
    "        validation_split=.20,\n",
    "        subset=subset,\n",
    "        label_mode=\"categorical\",\n",
    "        # Seed needs to provided when using validation_split and shuffle = True.\n",
    "        # A fixed seed is used so that the validation set is stable across runs.\n",
    "        seed=123,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        batch_size=32)\n",
    "\n",
    "train_ds = build_dataset(train_path, \"training\")\n",
    "class_names = tuple(train_ds.class_names)\n",
    "train_size = train_ds.cardinality().numpy()\n",
    "train_ds = train_ds.unbatch().batch(BATCH_SIZE)\n",
    "train_ds = train_ds.repeat()\n",
    "\n",
    "normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
    "preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
    "\n",
    "do_data_augmentation = True\n",
    "\n",
    "if do_data_augmentation:\n",
    "    preprocessing_model.add(tf.keras.layers.RandomRotation(40))\n",
    "    preprocessing_model.add(tf.keras.layers.RandomTranslation(0, 0.2))\n",
    "    preprocessing_model.add(tf.keras.layers.RandomTranslation(0.2, 0))\n",
    "    # Like the old tf.keras.preprocessing.image.ImageDataGenerator(),\n",
    "    # image sizes are fixed when reading, and then a random zoom is applied.\n",
    "    # If all training inputs are larger than image_size, one could also use\n",
    "    # RandomCrop with a batch size of 1 and rebatch later.\n",
    "    preprocessing_model.add(tf.keras.layers.RandomZoom(0.2, 0.2))\n",
    "    preprocessing_model.add(tf.keras.layers.RandomFlip(mode=\"horizontal\"))\n",
    "\n",
    "train_ds = train_ds.map(lambda images, labels:\n",
    "                        (preprocessing_model(images), labels))\n",
    "\n",
    "val_ds = build_dataset(train_path, \"validation\")\n",
    "valid_size = val_ds.cardinality().numpy()\n",
    "val_ds = val_ds.unbatch().batch(BATCH_SIZE)\n",
    "val_ds = val_ds.map(lambda images, labels:\n",
    "                    (normalization_layer(images), labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad748978",
   "metadata": {},
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0fc68ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "     |████████████████████████████████| 108 kB 3.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow_hub) (1.19.2)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from tensorflow_hub) (3.17.3)\n",
      "Requirement already satisfied: six>=1.9 in /Users/alecsharp/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.12.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43b2194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8257a8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: efficientnetv2-xl-21k : https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\n",
      "Input size (512, 512)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"efficientnetv2-xl-21k\" # @param ['efficientnetv2-s', 'efficientnetv2-m', 'efficientnetv2-l', 'efficientnetv2-s-21k', 'efficientnetv2-m-21k', 'efficientnetv2-l-21k', 'efficientnetv2-xl-21k', 'efficientnetv2-b0-21k', 'efficientnetv2-b1-21k', 'efficientnetv2-b2-21k', 'efficientnetv2-b3-21k', 'efficientnetv2-s-21k-ft1k', 'efficientnetv2-m-21k-ft1k', 'efficientnetv2-l-21k-ft1k', 'efficientnetv2-xl-21k-ft1k', 'efficientnetv2-b0-21k-ft1k', 'efficientnetv2-b1-21k-ft1k', 'efficientnetv2-b2-21k-ft1k', 'efficientnetv2-b3-21k-ft1k', 'efficientnetv2-b0', 'efficientnetv2-b1', 'efficientnetv2-b2', 'efficientnetv2-b3', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b2', 'efficientnet_b3', 'efficientnet_b4', 'efficientnet_b5', 'efficientnet_b6', 'efficientnet_b7', 'bit_s-r50x1', 'inception_v3', 'inception_resnet_v2', 'resnet_v1_50', 'resnet_v1_101', 'resnet_v1_152', 'resnet_v2_50', 'resnet_v2_101', 'resnet_v2_152', 'nasnet_large', 'nasnet_mobile', 'pnasnet_large', 'mobilenet_v2_100_224', 'mobilenet_v2_130_224', 'mobilenet_v2_140_224', 'mobilenet_v3_small_100_224', 'mobilenet_v3_small_075_224', 'mobilenet_v3_large_100_224', 'mobilenet_v3_large_075_224']\n",
    "\n",
    "model_handle_map = {\n",
    "  \"efficientnetv2-s\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-s-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b3/feature_vector/2\",\n",
    "  \"efficientnetv2-s-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_vector/2\",\n",
    "  \"efficientnetv2-m-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_m/feature_vector/2\",\n",
    "  \"efficientnetv2-l-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_l/feature_vector/2\",\n",
    "  \"efficientnetv2-xl-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_xl/feature_vector/2\",\n",
    "  \"efficientnetv2-b0-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3-21k-ft1k\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_b3/feature_vector/2\",\n",
    "  \"efficientnetv2-b0\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/feature_vector/2\",\n",
    "  \"efficientnetv2-b1\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b1/feature_vector/2\",\n",
    "  \"efficientnetv2-b2\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2\",\n",
    "  \"efficientnetv2-b3\": \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b3/feature_vector/2\",\n",
    "  \"efficientnet_b0\": \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\",\n",
    "  \"efficientnet_b1\": \"https://tfhub.dev/tensorflow/efficientnet/b1/feature-vector/1\",\n",
    "  \"efficientnet_b2\": \"https://tfhub.dev/tensorflow/efficientnet/b2/feature-vector/1\",\n",
    "  \"efficientnet_b3\": \"https://tfhub.dev/tensorflow/efficientnet/b3/feature-vector/1\",\n",
    "  \"efficientnet_b4\": \"https://tfhub.dev/tensorflow/efficientnet/b4/feature-vector/1\",\n",
    "  \"efficientnet_b5\": \"https://tfhub.dev/tensorflow/efficientnet/b5/feature-vector/1\",\n",
    "  \"efficientnet_b6\": \"https://tfhub.dev/tensorflow/efficientnet/b6/feature-vector/1\",\n",
    "  \"efficientnet_b7\": \"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\",\n",
    "  \"bit_s-r50x1\": \"https://tfhub.dev/google/bit/s-r50x1/1\",\n",
    "  \"inception_v3\": \"https://tfhub.dev/google/imagenet/inception_v3/feature-vector/4\",\n",
    "  \"inception_resnet_v2\": \"https://tfhub.dev/google/imagenet/inception_resnet_v2/feature-vector/4\",\n",
    "  \"resnet_v1_50\": \"https://tfhub.dev/google/imagenet/resnet_v1_50/feature-vector/4\",\n",
    "  \"resnet_v1_101\": \"https://tfhub.dev/google/imagenet/resnet_v1_101/feature-vector/4\",\n",
    "  \"resnet_v1_152\": \"https://tfhub.dev/google/imagenet/resnet_v1_152/feature-vector/4\",\n",
    "  \"resnet_v2_50\": \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature-vector/4\",\n",
    "  \"resnet_v2_101\": \"https://tfhub.dev/google/imagenet/resnet_v2_101/feature-vector/4\",\n",
    "  \"resnet_v2_152\": \"https://tfhub.dev/google/imagenet/resnet_v2_152/feature-vector/4\",\n",
    "  \"nasnet_large\": \"https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/4\",\n",
    "  \"nasnet_mobile\": \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\",\n",
    "  \"pnasnet_large\": \"https://tfhub.dev/google/imagenet/pnasnet_large/feature_vector/4\",\n",
    "  \"mobilenet_v2_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4\",\n",
    "  \"mobilenet_v2_130_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/feature_vector/4\",\n",
    "  \"mobilenet_v2_140_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/feature_vector/4\",\n",
    "  \"mobilenet_v3_small_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_small_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_large_100_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5\",\n",
    "  \"mobilenet_v3_large_075_224\": \"https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5\",\n",
    "}\n",
    "\n",
    "model_image_size_map = {\n",
    "  \"efficientnetv2-s\": 384,\n",
    "  \"efficientnetv2-m\": 480,\n",
    "  \"efficientnetv2-l\": 480,\n",
    "  \"efficientnetv2-b0\": 224,\n",
    "  \"efficientnetv2-b1\": 240,\n",
    "  \"efficientnetv2-b2\": 260,\n",
    "  \"efficientnetv2-b3\": 300,\n",
    "  \"efficientnetv2-s-21k\": 384,\n",
    "  \"efficientnetv2-m-21k\": 480,\n",
    "  \"efficientnetv2-l-21k\": 480,\n",
    "  \"efficientnetv2-xl-21k\": 512,\n",
    "  \"efficientnetv2-b0-21k\": 224,\n",
    "  \"efficientnetv2-b1-21k\": 240,\n",
    "  \"efficientnetv2-b2-21k\": 260,\n",
    "  \"efficientnetv2-b3-21k\": 300,\n",
    "  \"efficientnetv2-s-21k-ft1k\": 384,\n",
    "  \"efficientnetv2-m-21k-ft1k\": 480,\n",
    "  \"efficientnetv2-l-21k-ft1k\": 480,\n",
    "  \"efficientnetv2-xl-21k-ft1k\": 512,\n",
    "  \"efficientnetv2-b0-21k-ft1k\": 224,\n",
    "  \"efficientnetv2-b1-21k-ft1k\": 240,\n",
    "  \"efficientnetv2-b2-21k-ft1k\": 260,\n",
    "  \"efficientnetv2-b3-21k-ft1k\": 300, \n",
    "  \"efficientnet_b0\": 224,\n",
    "  \"efficientnet_b1\": 240,\n",
    "  \"efficientnet_b2\": 260,\n",
    "  \"efficientnet_b3\": 300,\n",
    "  \"efficientnet_b4\": 380,\n",
    "  \"efficientnet_b5\": 456,\n",
    "  \"efficientnet_b6\": 528,\n",
    "  \"efficientnet_b7\": 600,\n",
    "  \"inception_v3\": 299,\n",
    "  \"inception_resnet_v2\": 299,\n",
    "  \"nasnet_large\": 331,\n",
    "  \"pnasnet_large\": 331,\n",
    "}\n",
    "\n",
    "model_handle = model_handle_map.get(model_name)\n",
    "pixels = model_image_size_map.get(model_name, 224)\n",
    "\n",
    "print(f\"Selected model: {model_name} : {model_handle}\")\n",
    "\n",
    "IMAGE_SIZE = (pixels, pixels)\n",
    "print(f\"Input size {IMAGE_SIZE}\")\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab4e4fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = hub.load(model_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1d5d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_fine_tuning = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ccefcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_fine_tuning = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a91703c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_xl/feature_vector/2\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 1280)              207615832 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 6405      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 207,622,237\n",
      "Trainable params: 6,405\n",
      "Non-trainable params: 207,615,832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Building model with\", model_handle)\n",
    "model = tf.keras.Sequential([\n",
    "    # Explicitly define the input shape so the model can be properly\n",
    "    # loaded by the TFLiteConverter\n",
    "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
    "    hub.KerasLayer(model_handle, trainable=do_fine_tuning),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(num_classes,\n",
    "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
    "])\n",
    "model.build((None,)+IMAGE_SIZE+(3,))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8edb1914",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9), \n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1d8448e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow_hub.keras_layer.KerasLayer object at 0x153b03430> False\n",
      "<keras.layers.core.dropout.Dropout object at 0x1548c5910> False\n",
      "<keras.layers.core.dense.Dense object at 0x1548c5be0> False\n"
     ]
    }
   ],
   "source": [
    "# for layer in model.layers[:]:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# # Check the trainable status of the individual layers\n",
    "# for layer in model.layers:\n",
    "#     print(layer, layer.trainable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886870b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 1/16 [>.............................] - ETA: 37:02 - loss: 2.7073 - accuracy: 0.1562"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = train_size // BATCH_SIZE\n",
    "validation_steps = valid_size // BATCH_SIZE\n",
    "hist = model.fit(\n",
    "    train_ds,\n",
    "    epochs=10, steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=validation_steps).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef10d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.ylabel(\"Loss (training and validation)\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,2])\n",
    "plt.plot(hist[\"loss\"])\n",
    "plt.plot(hist[\"val_loss\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.ylabel(\"Accuracy (training and validation)\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylim([0,1])\n",
    "plt.plot(hist[\"accuracy\"])\n",
    "plt.plot(hist[\"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be9852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(val_ds))\n",
    "image = x[0, :, :, :]\n",
    "true_index = np.argmax(y[0])\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Expand the validation image to (1, 224, 224, 3) before predicting the label\n",
    "prediction_scores = model.predict(np.expand_dims(image, axis=0))\n",
    "predicted_index = np.argmax(prediction_scores)\n",
    "print(\"True label: \" + class_names[true_index])\n",
    "print(\"Predicted label: \" + class_names[predicted_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a new model instance.\n",
    "#model = create_model()\n",
    "#model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "# Save the entire model as a SavedModel.\n",
    "!mkdir -p saved_model\n",
    "model.save('initial_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0677a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
